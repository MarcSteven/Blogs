CoreML预测不准问题实践
1，pytorch模型中图像预测错误，多为图像预处理与机器学习模型不一致。在iOS中CVPixelBuffer通常包含了ARGB或者BGRA格式的像素，每个都是八位，这意味着像素值在图像中是0–255.在机器学习中普遍的选择大概有如下几种：
* 0 ~ 1
* -1 ~ 1
* - 2 ~ 2
* 在-128和+128之间，减去R、G和B的平均值
* 颜色通道按BGR顺序而不是RGB
如果你的模型期望像素值在0–255之间，你需要让Core ML知道
需要保证在CoreML中做同样的处理操作步骤，如果你的模型是从网络上下载的在他们进入第一层神经网络层之前必须确认清楚哪个预处理被操作。
2，确认输入是正确的
当你输入错误的时候，程序会抛出异常，只有传入符合标准的参数程序才能正常运行。
3，RGB 或者 BGR
大多数训练工具以RGB的形式来加载图像，但是Caffe加载图像是以BGR的形式加载，如果你的训练模型使用opencv加载图像，它也可能使用BGR像素格式，必要的时候CoreML能自动红色和蓝色通道，但是必须告诉它你的模型是否期望BGR或者RGB。
** 咱们的模型为RGB **
4，在转换的时候加入预处理参数
在转换的时候加入了预处理参数image_scale:1/255.0,以便像素值范围在0–1之间。按理说应该正常工作，但是并未正常工作。
5，在客户端对pixelBuffer进行标准化处理
与之前的结果并未差别
6，在模型中加入预处理操作，可以在forward中做预处理处理，但是技术上转换中onnx不会引入预处理的步骤
7，在mlmodel中手动加入scaler层进行预处理操作- 待验证
8，模型输入类型保持不变，采用multiArray作为输入，测试结果依然全部为sad。
9，模型输入类型更改为image，市面上都是这样做的，而且这是苹果官方推荐的做法，因为传入图像的channel为3，则将输入转换为image是不影响的。将模型输入类型改为image，更方便去在模型转换后进行验证，因为苹果官方提供的测试验证工具coremltools要求输入为image，结果返回一个dict，**multiArray的处理问题很多，这也是为什么苹果官方的demo都以image作为输入类型的真正原因**
10，删除无用的dimessions，让Core ML将数据解释为图像,如果在一个multiArray中有多于三个shape，第一个或者在最后一个是1， 你能删除它，对于事实上的数据并无影响- 已验证并无影响。
