# PyTorch学习笔记
在PyTorch中数据结构都是基于表和基于tensor
## 1，练习tensors和它的操作
1,检查是不是tensor对象
`torch.is_tensor(y)`
2,确认是否是一个tensor对象被存储
`torch.is_storage(y)`
3,在输入tensor中的元素总数
`torch.numel(y)`
4,在一个2D tensor中创建一个零值的tensor并数对应的元素数量
`torch.zeros(4,4)`
`torch.numel(torch.zeros(4,4))`
5,创建一个对角坐标 通过eye函数，numpy中也有这样的操作
## 2,Pytorch 实现CNN
在Pytorch中有可变的内建数据集，一般考虑MNIST数据集去构建一个CNN模型
步骤：
1，安装超参数
2，安装架构
3，训练模型和实现预测
## 再上传一个模型问题
如何存储和上传一个已经被训练的模型，给一个自然的深入学习模型，要求更长的训练时间，计算过程将会创建一个巨大的消耗，我们可以在训练模型（用一个新的输入）和存储模型？
解决方案：在一个生产环境，我们典型地不能同时训练和预测因为训练过程花了较长的时间，预测服务不能被应用直到使用epoch的训练过程被完成。预测服务不能被应用，不相关的来自预测过程的训练过程被要求，因此，我们需要存储应用的训练模型直到下一个训练的动作被完成后继续

## 安装一个loss函数问题-如何在pytorch内部安装一个loss函数并且优化它。选择正确的loss函数去提升模型的覆盖性
## activation 函数是什么？它们在项目中如何工作，如何实现一个activationg函数
激活函数是一个数学公式，它将二进制、浮点或整数格式的向量转换成另一种基于数学转换函数类型的格式。神经元存在于不同的层——输入层、隐藏层和输出层，这些层通过一个称为激活函数的数学函数相互连接。激活函数有不同的变体，下面将对此进行解释。理解激活函数有助于准确地实现神经网络模型。
所有属于神经网络模型的激活函数可以大致分为线性函数和非线性函数。PyTorch火炬。nn模块创建任何类型的神经网络模型。让我们看一些使用PyTorch和torch部署激活函数的示例。神经网络模块。
PyTorch和TensorFlow之间的核心区别在于计算图的定义方式、这两个框架执行计算的方式，以及我们在更改脚本和在其中引入其他基于python的库时所具有的灵活性。在TensorFlow中，我们需要在初始化模型之前定义变量和占位符。我们还需要跟踪稍后需要的对象，为此我们需要一个占位符。在TensorFlow中，我们需要首先定义模型，然后编译并运行;但是，在PyTorch中，我们可以按照自己的方式定义模型——我们不必在代码中保留占位符。这就是为什么PyTorch框架是动态的。
线性函数
将信息从demapping层传输到输出层。我们使用数据变化较小的地方的线性函数。在深度学习模型中，实践者通常在最后一个隐层到输出层使用一个线性函数。在线性函数中，输出总是被限制在一个特定的范围内;因此，它被用于深度学习模型的最后一个隐藏层，或用于基于线性回归的任务，或用于任务是预测来自输入数据集的结果的深度学习模型。下面是公式。 **在深入学习模型中，通常用一个线性函数在最后的隐藏层到输出层。**
`y = a +bx`

双线性函数： 双线性函数
双线性函数是用来传递信息的简单函数。它对输入的数据应用双线性变换。
`y = x1 * A * x2 + b`

sigmoid函数 - sigmoid函数经常被数据挖掘和分析领域的专业人士使用，因为它更容易解释和实现。它是一个非线性函数。当我们在神经网络中将权值从输入层传递到隐含层时，我们希望我们的模型能够捕捉数据中存在的各种非线性;因此，建议在神经网络的隐层中使用s形函数。非线性函数有助于数据集的泛化。用非线性函数计算函数的梯度比较容易。
sigmoid函数是一类特殊的非线性激活函数。sigmoid函数输出总是限制在0和1以内;因此，它主要用于执行基于分类的任务。sigmoid函数的一个局限性是它可能会陷入局部极小值。一个优点是它提供了属于这个类的概率。下面是它的方程。


双曲正切函数： 双曲正切函数是变换函数的另一种变体。它被用来将信息从映射层转换到隐藏层。它广泛应用于神经网络模型的隐藏层之间，正切函数的范围在-1和+1

对数x型函数：通常用于映射输入层到隐藏层。如果数据不是二进制的，而且它是一个有很多异常值的浮点类型(如输入特性中出现的大数值)，那么我们应该使用log sigmoid传递函数
Relu函数： rectified linear unit ,它主要用于从输入层到输出层之间传输信息。ReLu主要用于卷积神经网络模型。这个激活函数的作用范围是0到∞。它主要用于神经网络模型的不同隐层之间。不同类型的传递函数在神经网络结构中是可以互换的。它们可以在不同的阶段使用，如对隐含层的输入，对输出层的隐含层，等等，以提高模型的准确性。


漏水的ReLU
在标准的神经网络模型中，渐变问题是很常见的。为了避免这个问题，应用了泄漏ReLU。泄漏的ReLU允许一个小的和非零梯度时，单位是不活跃的。

## 可视化激活函数的形状
激活函数将数据从一层传输到另外一层，变换后的数据可以与实际的张量作图来形象化这个函数。我们取了一个样本张量，把它转换成一个PyTorch变量，应用这个函数，并把它存储为另一个张量。用matplotlib表示实际张量和变换后的张量。


正确选择激活函数不仅可以提供更好的准确性，还有助于提取有意义的信息
##  构建一个基本的神经网络模型
1，准备训练数据
2，初始化权重
3，创建一个基本的网络模型
4，计算损失函数
5，选择学习速率
6，根据模型参数优化损失函数

一旦我们定义了一个网络结构，然后我们需要将结果与输出进行比较，以评估预测步骤。跟踪系统精度的指标是损耗函数，我们希望它是最小的。损失函数可能有不同的形状。我们如何确切地知道损失在哪里是最小的，哪个对应哪个迭代提供了最好的结果?要知道这一点，我们需要把优化函数应用到损失函数上;它找出最小的损失值。然后我们可以提取出与该迭代相对应的参数

## Tensor不同问题
什么是张量微分，它与使用PyTorch框架执行计算图形有什么关系?

计算图网络由节点表示，通过函数连接。有两种不同的节点:依赖的和独立的。相关节点正在等待来自其他节点的结果来处理输入。相互连接的独立节点要么是常量，要么是结果。张量微分是计算图环境中进行计算的一种有效方法。
在计算图中，张量微分是非常有效的，因为张量可以被计算为并行节点、多进程节点或多线程节点。主要的深度学习和神经计算框架包括这个张量微分。
Autograd是帮助执行张量微分的函数，这意味着计算误差函数的梯度或斜率，并通过神经网络反向传播误差来微调权重和偏差。通过学习率和迭代，试图减少误差值或损失函数。
要应用张量微分，需要使用nn. backwards()方法。让我们举个例子，看看误差梯度是如何反向传播的。若要更新损失函数的曲线，或要找出损失函数的形状在何处最小以及它的移动方向，则需要进行导数计算。张量微分是在计算图形中计算函数斜率的一种方法。

## RNNs
RNNS用于时间系列的分析
当你需要基于原来的数据做一个预测的时候，而不是当前的数据，它充分应用序列信息


## 
在概率论和统计学中，随机变量也被称为随机变量，其结果取决于一个纯粹的随机现象，或随机现象。概率分布有不同的类型，包括正态分布、二项分布、多项分布和伯努利分布。每个统计分布都有自己的特性。
火炬。分布模块包括概率分布和采样函数。每种分布类型在计算图中都有自己的重要性。分布模块包括二项分布、伯努利分布、贝塔分布、分类分布、指数分布、正态分布和泊松分布。
##  样品sample tensor问题
权值初始化是训练神经网络和任何深度学习模型的重要任务，如卷积神经网络(CNN)、深度神经网络(DNN)和递归神经网络(RNN)。”问题总是在于如何初始化权重
权值初始化可以通过多种方法来完成，包括随机权值初始化。基于分布的权值初始化使用均匀分布、伯努利分布、多项分布和正态分布。接下来将介绍如何使用PyTorch实现这一点

要执行一个神经网络，需要将一组初始权值传递给反向传播层来计算损失函数(因此，精度可以计算出来)。方法的选择取决于数据类型、任务和模型所需的优化。

如果用户事件需要在产生一个同样的集合结果去维护一致性，然后一个手动的种子需要被设置

种子值能被自定义化，随机数一个偶然的机会去生成。随机数将从一个静态的派发中生成，这个可能的密度函数
在统计学中，伯努利分布被认为是离散的概率分布，二个可能的输出。 如果事件发生，然后值为1，如果事件没有发生，则值为0 

对于离散的概率分布，我们计算的是概率质量函数而不是概率密度函数。概率质量函数是这样的




## 2,使用anaconda去实时处理数据
`conda install -c peterjc123 pytorch`


## 模型部署到web云台还是设备上
运行在clound
* 1， 你需要有后台，如果你已经有了后台的话，你的预测逻辑则可以和已经存在的后台迭代
* 2，你可以使用相同的包去训练和预测-当你在本地预测的话则需要重写逻辑，例如iOS要写一套逻辑，安卓端要写一套预测逻辑，而web端也需要一套逻辑
* 3，模型更新可以在任何时间进行
* 4，所有的机器学习逻辑都在服务器上，则可以更快地切换到其他平台，例如iOS，安卓，web
* 5，因为你的业务逻辑都在服务器端加密了，所以你不要担心你的竞争对手来逆向你的软件来获取对应数据模型的相关参数
运行在设备上
* 1，App的尺寸会随着模型的增大而导致App的size越来越大
* 2，更新模型变得困难，用户必须要下载一个更新来改善模型
* 3，转换到不同的平台则变得不容易，每个平台都要重写一遍这样的预测逻辑
* 4，很容易被其他开发者在App bundle内逆向拿到模型数据，可以十分容易地通过对应模型的学习参数来复制。
deloy to device:需要进行的步骤
PyTorch ->onnx->CoreML



|  | 在设备上预测 | 基于云台的预测 |
| --- | --- | --- |
| 隐私 | 用户数据从不离开设备 | 数据可能会离开设备，可能需要采取额外的预防措施 |
| 资源 | 特定设备的资源(如处理能力和存储)可能会限制性能 | 基于云的资源更强大，存储也更丰富 |
| 延迟 | 较低的延迟增强了实施体验 | 异步通信和可用带宽会影响延迟 |
| 离线、在线 | 具备离线操作的能力对于运行在差的或不存在的网络基础设施上是一个加分项 | 要求网络必须链接 |
| 成本 | 电池使用情况，最终用户的模型下载时间 | 为终端用户提供数据传输带宽，为开发人员提供计算费用 |

Pytorch目前尚不支持android，需要做的方法是将模型转为onnx然后再转换为caffe

TensorFlow lite目前支持的use case：
1，图像分类-标识数百个对象，包括人，行为，动物，植物和地名
2，对象检测- 用bounding box来检测多个对象
3，姿态估计- 针对单人或者多人猜测可能，包含了简笔人物舞派对
4，智能回复-通常对于聊天消息的可能建议回复
5，分割-通过严格的定位精度和语义标签，精确定位物体的形状，并与人、地点、动物等进行训练
6,风格转换- 申请一个风格在输入的图片上去创建一个新的艺术照
7，文本分类- 分类免费文本到预定义的群
8，问题和回答-回答用户从给定的文本中的信息查询

## 安卓上实施机器学习
实际上，有两种方法可以在android中应用machine learning。

首先，在移动设备上应用大型深度学习模型的一个更实用的方法是在移动设备上部署一个预训练模型，或者在服务器上部署一个训练模型，然后通过internet向您的手机请求结果。

另一种方法是获取即时数据并直接在设备上训练模型。但是你只能在简单的模型和algrithom没有大型gpu计算。


[App上使用机器学习](https://stormotion.io/blog/what-is-machine-learning-and-how-to-use-it-in-your-mobile-app/)
## android机器学习TensorFlow
[安卓Tensorflow教程1](https://towardsdatascience.com/android-with-tensorflow-part-1-6897ba617b1e)
[安卓TensorFlow教程2 ](https://towardsdatascience.com/generativeadversarialnetwork-gan-in-android-tictactoe-ai-part-2-70155c5ca55b)
[使用TensorFlow lite构建一个安卓自定义的机器学习模型](https://medium.com/over-engineering/building-a-custom-machine-learning-model-on-android-with-tensorflow-lite-26447e53abf2)
## PyTorch 做一个图片分类
[使用PyTorch做一个人工智能图片分类](https://towardsdatascience.com/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7)




## 机器学习基础
损失分数常常用于作为一个反馈信号去调整权重
一层对其输入数据做什么的规范存储在该层的权值中，权值本质上是一串数字。从技术上讲，我们可以说一个层实现的转换是由它的权值参数化的
无向量 0D tensors
一个tensor包含一个数叫做无向量
vectors -1D tensor
一组数字被称为矢量，或一维张量。一个一维张量只有一个轴

Matrix - 一组张量的数组就是坐标
一个matrix有二个轴-行和列
tensor的三个关键属性
轴的数量 -一个3D张量有三个轴，一个坐标有2个轴，这称之为tensor的ndim
Shape- 整数的原则，描述围绕每个轴有多少个维
数据类型

随机变量- 其结果完全取决于随机现象

概率分布有不同的类型，包括正态分布，二项分布，多项分布和伯努利分布
每个统计分布都有自己的特性 -详见torch.distributions 

CNN- convolutional neural network 卷积神经网络
DNN- deep neural network 深层神经网络

RNN—— recurrent neural network 递归神经网络

权值初始化可以通过多种方法来完成，包括随机权值初始化。利用均匀分布、伯努利分布、多项分布和正态分布进行基于分布的权值初始化。

要执行一个神经网络，需要将一组初始权值传递给反向传播层来计算损失函数(因此，精度可以计算出来)。方法的选择取决于数据类型，任务，以及对于模型的优化

* 1 ，如果用例需要复制相同的结果集来保持一致性，那么需要设置手动种子，种子值能被自定义化，随机数随机产生。随机数从一个统计分布生成。
Tensor 在变量内被交换，有三种属性：grad, volatile gradient


* 2，change the multiArray to image and then output the result as the related 
* 

